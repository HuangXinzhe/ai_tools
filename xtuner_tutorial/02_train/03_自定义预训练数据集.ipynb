{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义预训练数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"text\": \"xxx\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"xxx\"\n",
    "    },\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "- 每条 text 数据不要太长（分词个数应小于 max_length），以避免在数据处理阶段被截断。\n",
    "- 为保证数据上下文的一致性，请确保长文本数据在被切分为多个 text 后，json 列表的顺序与实际上下文顺序一致。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导出config\n",
    "xtuner/configs/custom_dataset/pretrain/ 目录下有所有支持预训练的配置文件  \n",
    "xtuner copy-cfg internlm2_7b_full_custom_pretrain_e1 .  \n",
    "将选用的配置文件复制到当前目录下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 修改配置文件\n",
    "```\n",
    "- data_files = ['/path/to/json/file.json']\n",
    "+ data_files = ['/path/to/custom_dataset1.json', '/path/to/custom_dataset2.json', ...]\n",
    "```\n",
    "目录下的所有数据\n",
    "```\n",
    "#######################################################################\n",
    "#                          PART 1  Settings                           #\n",
    "#######################################################################\n",
    "# Data\n",
    "- data_files = ['/path/to/json/file.json']\n",
    "+ data_dir = '/dir/to/custom_dataset'\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 3  Dataset & Dataloader                   #\n",
    "#######################################################################\n",
    "train_dataset = dict(\n",
    "-   dataset=dict(type=load_dataset, path='json', data_files=data_files),\n",
    "+   dataset=dict(type=load_dataset, path='json', data_dir=data_dir),\n",
    "    ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lora\n",
    "```\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True),\n",
    "+   lora=dict(\n",
    "+       type=LoraConfig,\n",
    "+       r=64,\n",
    "+       lora_alpha=16,\n",
    "+       lora_dropout=0.1,\n",
    "+       bias='none',\n",
    "+       task_type='CAUSAL_LM'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qlora\n",
    "```\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "+       quantization_config=dict(\n",
    "+           type=BitsAndBytesConfig,\n",
    "+           load_in_4bit=True,\n",
    "+           load_in_8bit=False,\n",
    "+           llm_int8_threshold=6.0,\n",
    "+           llm_int8_has_fp16_weight=False,\n",
    "+           bnb_4bit_compute_dtype=torch.float16,\n",
    "+           bnb_4bit_use_double_quant=True,\n",
    "+           bnb_4bit_quant_type='nf4')\n",
    "    ),\n",
    "+   lora=dict(\n",
    "+       type=LoraConfig,\n",
    "+       r=64,\n",
    "+       lora_alpha=16,\n",
    "+       lora_dropout=0.1,\n",
    "+       bias='none',\n",
    "+       task_type='CAUSAL_LM')\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 开始训练\n",
    "NPROC_PER_NODE=8 xtuner train internlm2_7b_full_custom_pretrain_e1_copy.py --deepspeed deepspeed_zero1  \n",
    "训得模型将默认保存在 ./work_dirs/，用户可以通过命令 xtuner train --work-dir ${SAVE_PATH} 指定保存路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 模型转换\n",
    "```\n",
    "xtuner convert pth_to_hf ${FINETUNE_CFG} ${PTH_PATH} ${SAVE_PATH}\n",
    "# 例如：xtuner convert pth_to_hf internlm2_7b_full_custom_pretrain_e1_copy.py ./iter_2000.pth ./iter_2000_hf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话\n",
    "全量参数微调\n",
    "```\n",
    "xtuner chat ${PATH_TO_LLM} [optional arguments]\n",
    "# 例如：xtuner chat ./iter_2000_hf --max-new-tokens 512\n",
    "```\n",
    "lora或qlora微调\n",
    "```\n",
    "xtuner chat ${NAME_OR_PATH_TO_LLM} --adapter {NAME_OR_PATH_TO_ADAPTER} [optional arguments]\n",
    "# 例如：xtuner chat internlm/internlm2-7b --adapter ./iter_2000_hf --max-new-tokens 512\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型合并\n",
    "```\n",
    "(LLM) xtuner convert merge ${LLM} ${LLM_ADAPTER} ${SAVE_PATH}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评测\n",
    "OpenCompass  \n",
    "https://github.com/InternLM/opencompass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
