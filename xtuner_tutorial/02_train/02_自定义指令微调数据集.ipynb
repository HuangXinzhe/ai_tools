{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义指令微调数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "OpenAI SFT数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"xxx.\"},\n",
    "        {\"role\": \"user\", \"content\": \"xxx.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"xxx.\"}\n",
    "    ]\n",
    "},\n",
    "    {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"xxx.\"},\n",
    "        {\"role\": \"user\", \"content\": \"xxx.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"xxx.\", \"loss\": False},\n",
    "        {\"role\": \"user\", \"content\": \"xxx.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"xxx.\", \"loss\": True}\n",
    "    ]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss字段，用于控制某轮assistant的输出不计算loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system 和 user 的 loss 默认为 False\n",
    "\n",
    "assistant 的 loss 默认为 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出config\n",
    "xtuner copy-cfg internlm2_chat_7b_qlora_custom_sft_e1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- data_files = ['/path/to/json/file.json']\n",
    "+ data_files = ['/path/to/custom_sft1.json', '/path/to/custom_sft2.json', ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用某个目录下所有的 json 文件作为训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                          PART 1  Settings                           #\n",
    "#######################################################################\n",
    "# Data\n",
    "- data_files = ['/path/to/json/file.json']\n",
    "+ data_dir = '/dir/to/custom_sft'\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 3  Dataset & Dataloader                   #\n",
    "#######################################################################\n",
    "train_dataset = dict(\n",
    "-   dataset=dict(type=load_dataset, path='json', data_files=data_files),\n",
    "+   dataset=dict(type=load_dataset, path='json', data_dir=data_dir),\n",
    "    ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lora训练\n",
    "```\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "-       quantization_config=dict(\n",
    "-           type=BitsAndBytesConfig,\n",
    "-           load_in_4bit=True,\n",
    "-           load_in_8bit=False,\n",
    "-           llm_int8_threshold=6.0,\n",
    "-           llm_int8_has_fp16_weight=False,\n",
    "-           bnb_4bit_compute_dtype=torch.float16,\n",
    "-           bnb_4bit_use_double_quant=True,\n",
    "-           bnb_4bit_quant_type='nf4')\n",
    "    ),\n",
    "    lora=dict(\n",
    "        type=LoraConfig,\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全量参数训练\n",
    "```\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "-       quantization_config=dict(\n",
    "-           type=BitsAndBytesConfig,\n",
    "-           load_in_4bit=True,\n",
    "-           load_in_8bit=False,\n",
    "-           llm_int8_threshold=6.0,\n",
    "-           llm_int8_has_fp16_weight=False,\n",
    "-           bnb_4bit_compute_dtype=torch.float16,\n",
    "-           bnb_4bit_use_double_quant=True,\n",
    "-           bnb_4bit_quant_type='nf4')\n",
    "    ),\n",
    "-   lora=dict(\n",
    "-       type=LoraConfig,\n",
    "-       r=64,\n",
    "-       lora_alpha=16,\n",
    "-       lora_dropout=0.1,\n",
    "-       bias='none',\n",
    "-       task_type='CAUSAL_LM')\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPROC_PER_NODE=8 xtuner train internlm2_chat_7b_qlora_custom_sft_e1_copy.py --deepspeed deepspeed_zero1\n",
    "\n",
    "# 训练日志及 checkpoint 将默认保存在 ./work_dirs/，可以通过命令 xtuner train --work-dir ${SAVE_PATH} 指定保存路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型转换\n",
    "```\n",
    "xtuner convert pth_to_hf ${FINETUNE_CFG} ${PTH_PATH} ${SAVE_PATH}\n",
    "# 例如：xtuner convert pth_to_hf internlm2_chat_7b_qlora_custom_sft_e1_copy.py ./iter_2000.pth ./iter_2000_hf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话\n",
    "```\n",
    "xtuner chat ${NAME_OR_PATH_TO_LLM} --adapter {NAME_OR_PATH_TO_ADAPTER} --prompt-template ${PROMPT_TEMPLATE} [optional arguments]\n",
    "# 例如：xtuner chat internlm/internlm2-7b --adapter ./iter_2000_hf --prompt-template internlm2_chat\n",
    "```\n",
    "全量参数训练\n",
    "```\n",
    "xtuner chat ${PATH_TO_LLM} --prompt-template ${PROMPT_TEMPLATE} [optional arguments]\n",
    "# 例如：xtuner chat ./iter_2000_hf --prompt-template internlm2_chat\n",
    "```\n",
    "其中 ${PROMPT_TEMPLATE} 表示模型的对话模板，需要与训练用的 config 中的 prompt_template 字段保持一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型合并\n",
    "```\n",
    "xtuner convert merge ${LLM} ${LLM_ADAPTER} ${SAVE_PATH}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评测\n",
    "OpenCompass  \n",
    "https://github.com/InternLM/opencompass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
