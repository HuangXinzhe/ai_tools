{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载模型的各种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id='internlm/internlm2-chat-7b',\n",
    "                  local_dir='./internlm2-chat-7b',   # 本地保存路径\n",
    "                  max_workers=20)\n",
    "\n",
    "\"\"\"\n",
    "# 默认为 `~/.cache/huggingface/`\n",
    "export HF_HOME=XXXX\n",
    "\n",
    "如果觉得下载较慢（例如无法达到最大带宽等情况），可以尝试设置export HF_HUB_ENABLE_HF_TRANSFER=1 以获得更高的下载速度。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "snapshot_download(model_id='Shanghai_AI_Laboratory/internlm2-chat-7b',\n",
    "                  cache_dir='./internlm2-chat-7b')  # 本地保存路径\n",
    "\n",
    "\"\"\"\n",
    "# 默认为 ~/.cache/modelscope/hub/\n",
    "export MODELSCOPE_CACHE=XXXX\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git LFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git lfs install\n",
    "# From HuggingFace\n",
    "git clone https://huggingface.co/internlm/internlm2-chat-7b\n",
    "# From ModelScope\n",
    "git clone https://www.modelscope.cn/Shanghai_AI_Laboratory/internlm2-chat-7b.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('internlm/internlm2-chat-7b', \n",
    "                                             trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('internlm/internlm2-chat-7b', \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "\"\"\"\n",
    "# 默认为 `~/.cache/huggingface/`\n",
    "export HF_HOME=XXXX\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('Shanghai_AI_Laboratory/internlm2-chat-7b', \n",
    "                                             trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('Shanghai_AI_Laboratory/internlm2-chat-7b', \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "\"\"\"\n",
    "# 默认为 ~/.cache/modelscope/hub/\n",
    "export MODELSCOPE_CACHE=XXXX\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
